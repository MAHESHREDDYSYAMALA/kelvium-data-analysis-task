import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import csv
import os


base_url = 'https://results.eci.gov.in/'


def fetch_and_parse(url):
    response = requests.get(url)
    if response.status_code == 200:
        return BeautifulSoup(response.content, 'html.parser')
    else:
        print(f'Failed to fetch {url}, Status code: {response.status_code}')
        return None


base_soup = fetch_and_parse(base_url)

if base_soup:
  
    links = base_soup.find_all('a', href=True)

    
    page_urls = []


    for link in links:
        href = link['href']
        absolute_url = urljoin(base_url, href)  # Resolve relative URLs
        if absolute_url.startswith(base_url):
            if 'PcResultGenJune2024' in absolute_url or 'AcResultGenJune2024' in absolute_url or 'AcResultByeJune2024' in absolute_url or 'AcResultGen2ndJune2024' in absolute_url:
                page_urls.append(absolute_url)

   
    downloads_path = os.path.expanduser('~/Downloads')

    
    csv_filename = os.path.join(downloads_path, 'election_results.csv')

  
    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
        csv_writer = csv.writer(csvfile)


        headers_written = False

       
        for url in page_urls:
            print(f"Scraping data from: {url}")
            soup = fetch_and_parse(url)
            if soup:
               
                tables = soup.find_all('table')
                for table in tables:
                    rows = table.find_all('tr')
                    for i, row in enumerate(rows):
                        cells = [cell.text.strip() for cell in row.find_all(['td', 'th'])]
                        # Skip the unwanted header row and the first row if it matches the unwanted format
                        if cells == ['Party', 'Won', 'Leading', 'Total', 'Total', '543', '0', '543'] or (i == 0 and cells == ['Party', 'Won', 'Leading', 'Total']):
                            continue
                        if cells:
                            if not headers_written:
                                csv_writer.writerow(['Party', 'Won', 'Leading', 'Total'])  # Write header row only once
                                headers_written = True
                            csv_writer.writerow(cells)  # Write data row

    print(f"Data successfully written to {csv_filename}")

else:
    print("Failed to fetch base URL.")
